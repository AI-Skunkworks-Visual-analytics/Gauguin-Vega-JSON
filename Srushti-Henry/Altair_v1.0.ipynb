{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altair - Vega Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altair is a declarative statistical visualization library for Python, based on Vega and Vega-Lite.\n",
    "Altair offers a powerful and concise visualization grammar that enables you to build a wide range of statistical visualizations quickly.\n",
    "\n",
    "https://altair-viz.github.io/getting_started/overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying Data in Altair\n",
    "\n",
    "Each top-level chart object (i.e. Chart, LayerChart, and VConcatChart, HConcatChart, RepeatChart, FacetChart) accepts a dataset as its first argument. The dataset can be specified in one of the following ways:\n",
    "\n",
    "    as a Pandas DataFrame\n",
    "    as a Data or related object (i.e. UrlData, InlineData, NamedData)\n",
    "    as a url string pointing to a json or csv formatted text file\n",
    "    as an object that supports the __geo_interface__ (eg. Geopandas GeoDataFrame, Shapely Geometries, GeoJSON Objects)\n",
    "  \n",
    "https://altair-viz.github.io/user_guide/data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The motive of this exercise is to generate as many graphs as possible from the given dataset using Altair. Currently we are focusing to generate graph with all the available columns (except for string data types). We are automating the process of JSON generation which will then be used to generate image in Visual Studio code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "#import  Altair API  \n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('Titanic_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AB_NYC_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating null values\n",
    "nulls_count = {col: df[col].isnull().sum() for col in df.columns} \n",
    "print(nulls_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are dropping  Columns which have more than 30% of null value\n",
    "# Repalcing null value with mean in case of int and float\n",
    "# If null value persist for other cases we are dropping those rows\n",
    "\n",
    "is_null_count_out_of_range = {col: df[col].isnull().sum()/df.shape[0] *100 > 30 for col in df.columns}\n",
    "\n",
    "for k,v in is_null_count_out_of_range.items():\n",
    "    if v:\n",
    "        df.drop( k,axis=1,inplace=True )\n",
    "    else:\n",
    "        if isinstance(df[k][0], (np.int64, np.float64)) :\n",
    "            df[k].fillna(df[k].mean(), inplace=True)\n",
    "        else :\n",
    "            drop_list = df[df[k].isnull()].index.tolist()\n",
    "            df.drop( drop_list,axis=0,inplace=True  )\n",
    "            \n",
    "     \n",
    "    nulls_count = {col: df[col].isnull().sum() for col in df.columns}\n",
    "    \n",
    "print(nulls_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Finding all the unique values to in each column\n",
    "uniques = {col: df[col].unique().tolist() for col in df.columns}\n",
    "\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory \n",
    "directory = \"Altair_Plots\"\n",
    "  \n",
    "# Parent Directory path \n",
    "parent_dir = \"../\"\n",
    "  \n",
    "# Path \n",
    "path = os.path.join(parent_dir, directory) \n",
    "\n",
    "try:  \n",
    "    os.mkdir(path)  \n",
    "except OSError as error:  \n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the above created dictionary to a text file\n",
    "\n",
    "with open(path +'/Unique_values.txt', 'w') as json_file:\n",
    "      json.dump(uniques, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null value\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying number of unique values in each column \n",
    "for k,v in uniques.items():\n",
    "    v = pd.Index(v)\n",
    "    print(k +' : '+ str(len(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting columns to categorical having less than or equal to 10 \n",
    "#unique values in a cloumn\n",
    "\n",
    "for k,v in uniques.items():\n",
    "    if len(pd.Index(v)) <=10:\n",
    "        df[k]=df[k].astype('category')\n",
    "        \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col=df.select_dtypes(include=['category']).columns.tolist()\n",
    "num_col=df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Column : '\\n'\"+str(num_col))\n",
    "print(\"Categorical Column : '\\n'\"+str(cat_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altair JSON generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df).mark_bar().encode( x='neighbourhood_group',y='reviews_per_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating JSON using altair methods\n",
    "\n",
    "for i in range(len(cat_col)):\n",
    "    for j in range(len(num_col)): \n",
    "        chart=alt.Chart(df).mark_bar().encode( x=cat_col[i],y=num_col[j])\n",
    "        chart.save(path+'/'+str(cat_col[i])+\" Vs \"+str(num_col[j])+\"_\"+\"plot.json\")\n",
    "        #print(cat_col[i],num_col[j])\n",
    "print(\"JSON generated in \"\"Altair_Plots\"\" folder for the combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Copyright</b> 2020 Srushti Dhamangaonkar and Hung-Chih Huang<br>\n",
    "    <br>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:<br>\n",
    "    <br>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.<br>\n",
    "    <br>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "    <br><br>\n",
    "    \n",
    "<div class=\"text-center\">\n",
    "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/3.0/us/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\">Creative Commons Attribution 3.0 United States License</a>.<br>\n",
    "</div></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
